---
title: Online Learning with Regularized Knowledge Gradients
authors:
- admin
- Warren B. Powell
date: '2022-05-18'
publishDate: '2022-05-22T13:56:30.277137Z'
publication_types:
- paper-conference
publication: '*Advances in Knowledge Discovery and Data Mining: 26th Pacific-Asia
  Conference, PAKDD 2022, Chengdu, China, May 16â€“19, 2022, Proceedings, Part II*'
publication_short: In *PAKDD 2022*


doi: 10.1007/978-3-031-05936-0_26
abstract: We introduce a simple and effective regularization of knowledge gradient
  (KG) and use it to present the first sublinear regret bound result for KG-based
  algorithms. We construct online learning with regularized knowledge gradients (ORKG)
  algorithm with independent Gaussian belief model, and prove that ORKG algorithm
  achieves sublinear regret upper bound with high probability facing bounded independent
  Gaussian multi-armed bandit (MAB) problems. The theoretical properties of regularized
  KG and ORKG algorithm are analyzed, and the empirical characteristics of ORKG algorithm
  are empirically validated with MAB benchmark simulations. ORKG algorithm shows top-tier
  performance comparable to select MAB algorithms with provable regret bounds.
tags:
- Knowledge gradient
- Online learning
- Regret analysis

image:
  caption: 'Sensitivity analysis of ORKG shows graceful exploitation-exploration tradeoff'
  focal_point: ''
  preview_only: false

links:
- name: URL
  url: https://doi.org/10.1007/978-3-031-05936-0_26
url_pdf: 'papers/lee_powell_ORKG_2022.pdf'

featured: true

---
