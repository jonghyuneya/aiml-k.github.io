---
# Display name
title: Suhyun Bae

# Full Name (for SEO)
first_name: Suhyun
last_name: Bae

# Is this the primary user of the site?
superuser: false

# Role/position
role: M.S. Student

# Organizations/Affiliations
organizations:
  - name: Korea University
    url: 'http://www.korea.edu'

# Short bio (displayed in user profile at end of posts)
bio: The cake is not a lie!

interests:
- LLM & NLP
- Agent & LAM
- Model Architecture


education:
  courses:
    # - course: Ph.D. in Computer Science
    #   institution: Princeton University
    #   year: 2019
    - course: M.S. in Mathematics
      institution: Korea University
      year: 2026 (expected)
    - course: B.S. in Mathematics
      institution: Korea University
      year: 2024

# Social/Academic Networking
# For available icons, see: https://docs.hugoblox.com/getting-started/page-builder/#icons
#   For an email link, use "fas" icon pack, "envelope" icon, and a link in the
#   form "mailto:your-email@example.com" or "#contact" for contact widget.
social:
  - icon: envelope
    icon_pack: fas
    link: 'mailto:baeshstar@korea.ac.kr'
  - icon: github
    icon_pack: fab
    link: https://github.com/Ricepunchb
  - icon: linkedin
    icon_pack: fab
    link: https://www.linkedin.com/in/suhyun-bae-869043286
  - icon: cv
    icon_pack: ai
    link: cv/CV_SuHyunBae_2503.pdf
  # - icon: twitter
  #   icon_pack: fab
  #   link: https://twitter.com/GeorgeCushen
  # - icon: google-scholar
  #   icon_pack: ai
  #   link: https://scholar.google.com/citations?user=I-XhaYgAAAAJ
  # - icon: orcid
  #   icon_pack: ai
  #   link: https://orcid.org/0009-0002-7449-5336


# Enter email to display Gravatar (if Gravatar enabled in Config)
email: ''

# Highlight the author in author lists? (true/false)
highlight_name: false

# Organizational groups that you belong to (for People widget)
#   Set this to `[]` or comment out if you are not using People widget.
user_groups:
  - M.S. Students
---

<!-- 짧은 자기소개 -->
Hi, my name is SuHyun Bae. I'm Master student in Mathematical Data Science at Korea University. Also a researcher of AIML@K.
<!-- 연구분야/주제 관심사 소개 -->
My research interests are mainly in NLP and utilizing Language Models.
<!-- 그 외의 것/trivia -->
I have been interested in creating friendly AI since I was young. So I decided to study mathematics to understand the mathematical principles underlying AI. My goal is to analyze and design models rigorously.

### Research Experience

- 2023.07 - 2023.12 | **AI Grand Challenge 2023**    
Generated and refined a dataset using `GPT 3.5` via `openai-API` to fine-tune a model for answering multi-answer questions. Our lab won 7th place

- 2024.02 - 03 | **Practicing RAG**   
  Conducted research on Retrieval-Augmented Generation (RAG) technology and applied it to implement a model for the DACON competition, "한솔데코 도배하자 Q&A"

- 2024.03 - 06 | **KCC 2024 and Hallucination**   
  Classified hallucination types in QA task into 5 categories and identified that the natural language evaluation metrics BLEU, METEOR, and ROUGE each excel at detecting certain types of hallucinations while struggling with others. This was published at KCC 2024

- 2024.07 - 2025.01 | **Internship at SK Magic**   
  Product planning and development at SK magic, focusing on algorithm development for new product and researching on-device LLM ecosystems and data infrastructure for product integration

- 2025.03 - PRESENT | **Grokking and Hallucination**   
  Interested in the phenomenon of grokking and researching its potential role in reducing hallucinations
  

### Research Focus

My research focus is on understanding the phenomenon of grokking in language models and progressively expanding its application.
First, grokking has primarily been observed in domains such as modular arithmetic and group operations. I aim to extend this to logical structures that use natural language, such as syllogisms, to determine whether grokking can occur in these contexts.
Second, current studies on grokking have been largely limited to simple transformer-based models. I plan to investigate whether grokking can also be observed in small-scale language models (sLLMs) such as `LLaMA 3.2 1B` and `DeepSeek 1.78B`.


### Research Goal

My ultimate research goal is to verify that Language Models can generalize certain computations and logical structures through the phenomenon of grokking. 
Furthermore, I aim to demonstrate that this understanding can significantly reduce hallucination when Language Models generate answers using reasoning chains such as Chain of Thought (CoT) or induction.
